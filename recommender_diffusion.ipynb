{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hemit/anaconda3/envs/edge/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from pathlib import Path\n",
    "from functools import partial, wraps\n",
    "from collections import namedtuple\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "import torch\n",
    "from torch import nn, einsum\n",
    "from torch.cuda.amp import autocast\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torch.optim import Adam\n",
    "\n",
    "from torch_geometric.datasets import MovieLens1M\n",
    "from torch_geometric.data import HeteroData\n",
    "\n",
    "from einops import rearrange, reduce, repeat\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "from ema_pytorch import EMA\n",
    "\n",
    "from accelerate import Accelerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "def default(val, d):\n",
    "    if exists(val):\n",
    "        return val\n",
    "    return d() if callable(d) else d\n",
    "\n",
    "def once(fn):\n",
    "    called = False\n",
    "    @wraps(fn)\n",
    "    def inner(x):\n",
    "        nonlocal called\n",
    "        if called:\n",
    "            return\n",
    "        called = True\n",
    "        return fn(x)\n",
    "    return inner\n",
    "\n",
    "print_once = once(print)\n",
    "\n",
    "def cast_tuple(t, length = 1):\n",
    "    if isinstance(t, tuple):\n",
    "        return t\n",
    "    return ((t,) * length)\n",
    "\n",
    "def divisible_by(numer, denom):\n",
    "    return (numer % denom) == 0\n",
    "\n",
    "def identity(t, *args, **kwargs):\n",
    "    return t\n",
    "\n",
    "def cycle(dl):\n",
    "    while True:\n",
    "        for data in dl:\n",
    "            yield data\n",
    "\n",
    "def has_int_squareroot(num):\n",
    "    return (math.sqrt(num) ** 2) == num\n",
    "\n",
    "def num_to_groups(num, divisor):\n",
    "    groups = num // divisor\n",
    "    remainder = num % divisor\n",
    "    arr = [divisor] * groups\n",
    "    if remainder > 0:\n",
    "        arr.append(remainder)\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ATTENTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "AttentionConfig = namedtuple('AttentionConfig', ['enable_flash', 'enable_math', 'enable_mem_efficient'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attend(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dropout = 0.,\n",
    "        flash = False,\n",
    "        scale = None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        self.scale = scale\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.flash = flash\n",
    "\n",
    "        # determine efficient attention configs for cuda and cpu\n",
    "\n",
    "        self.cpu_config = AttentionConfig(True, True, True)\n",
    "        self.cuda_config = None\n",
    "\n",
    "        if not torch.cuda.is_available() or not flash:\n",
    "            return\n",
    "\n",
    "        device_properties = torch.cuda.get_device_properties(torch.device('cuda'))\n",
    "\n",
    "        if device_properties.major == 8 and device_properties.minor == 0:\n",
    "            print_once('A100 GPU detected, using flash attention if input tensor is on cuda')\n",
    "            self.cuda_config = AttentionConfig(True, False, False)\n",
    "        else:\n",
    "            print_once('Non-A100 GPU detected, using math or mem efficient attention if input tensor is on cuda')\n",
    "            self.cuda_config = AttentionConfig(False, True, True)\n",
    "\n",
    "    def flash_attn(self, q, k, v):\n",
    "        _, heads, q_len, _, k_len, is_cuda, device = *q.shape, k.shape[-2], q.is_cuda, q.device\n",
    "\n",
    "        if exists(self.scale):\n",
    "            default_scale = q.shape[-1]\n",
    "            q = q * (self.scale / default_scale)\n",
    "\n",
    "        q, k, v = map(lambda t: t.contiguous(), (q, k, v))\n",
    "\n",
    "        # Check if there is a compatible device for flash attention\n",
    "\n",
    "        config = self.cuda_config if is_cuda else self.cpu_config\n",
    "\n",
    "        # pytorch 2.0 flash attn: q, k, v, mask, dropout, causal, softmax_scale\n",
    "\n",
    "        with torch.backends.cuda.sdp_kernel(**config._asdict()):\n",
    "            out = F.scaled_dot_product_attention(\n",
    "                q, k, v,\n",
    "                dropout_p = self.dropout if self.training else 0.\n",
    "            )\n",
    "\n",
    "        return out\n",
    "\n",
    "    def forward(self, q, k, v):\n",
    "        \"\"\"\n",
    "        einstein notation\n",
    "        b - batch\n",
    "        h - heads\n",
    "        n, i, j - sequence length (base sequence length, source, target)\n",
    "        d - feature dimension\n",
    "        \"\"\"\n",
    "\n",
    "        q_len, k_len, device = q.shape[-2], k.shape[-2], q.device\n",
    "\n",
    "        if self.flash:\n",
    "            return self.flash_attn(q, k, v)\n",
    "\n",
    "        scale = default(self.scale, q.shape[-1] ** -0.5)\n",
    "\n",
    "        # similarity\n",
    "\n",
    "        sim = einsum(f\"b h i d, b h j d -> b h i j\", q, k) * scale\n",
    "\n",
    "        # attention\n",
    "\n",
    "        attn = sim.softmax(dim = -1)\n",
    "        attn = self.attn_dropout(attn)\n",
    "\n",
    "        # aggregate values\n",
    "\n",
    "        out = einsum(f\"b h i j, b h j d -> b h i d\", attn, v)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### row attention (per user or movie adjacency list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RowAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        heads = 4,\n",
    "        dim_head = 32,\n",
    "        num_mem_kv = 128,\n",
    "        flash = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        hidden_dim = dim_head * heads\n",
    "\n",
    "        self.attend = Attend(flash = flash)\n",
    "\n",
    "        self.mem_kv = nn.Parameter(torch.randn(2, heads, num_mem_kv, dim_head))\n",
    "        self.to_qkv = nn.Conv2d(\n",
    "            in_channels = dim, \n",
    "            out_channels = hidden_dim * 3, \n",
    "            kernel_size = 1, \n",
    "            bias = False)\n",
    "        self.to_out = nn.Conv2d(\n",
    "            in_channels = hidden_dim, \n",
    "            out_channels = dim, \n",
    "            kernel_size = 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        \n",
    "        qkv = self.to_qkv(x).chunk(3, dim = 1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b (h d) x y -> (b x) h y d', h = self.heads), qkv)\n",
    "\n",
    "        mk, mv = map(lambda t: repeat(t, 'h n d -> b h n d', b=b*h), self.mem_kv)\n",
    "        k, v = map(partial(torch.cat, dim=-2), ((mk, k), (mv, v)))\n",
    "\n",
    "        out = self.attend(q, k, v)\n",
    "\n",
    "        out = rearrange(out, '(b x) h y d -> b (h d) x y', x = h, y = w)\n",
    "        return self.to_out(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GUASSIAN DIFFUSION PIPELINE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### helpers and beta schedules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_to_neg_one_to_one(img):\n",
    "    return img * 2 - 1\n",
    "\n",
    "def unnormalize_to_zero_to_one(t):\n",
    "    return (t + 1) * 0.5\n",
    "\n",
    "def extract(a, t, x_shape):\n",
    "    b, *_ = t.shape\n",
    "    out = a.gather(-1, t)\n",
    "    return out.reshape(b, *((1,) * (len(x_shape) - 1)))\n",
    "\n",
    "def linear_beta_schedule(timesteps):\n",
    "    \"\"\"\n",
    "    linear schedule, proposed in original ddpm paper\n",
    "    \"\"\"\n",
    "    scale = 1000 / timesteps\n",
    "    beta_start = scale * 0.0001\n",
    "    beta_end = scale * 0.02\n",
    "    return torch.linspace(beta_start, beta_end, timesteps, dtype = torch.float64)\n",
    "\n",
    "def cosine_beta_schedule(timesteps, s = 0.008):\n",
    "    \"\"\"\n",
    "    cosine schedule\n",
    "    as proposed in https://openreview.net/forum?id=-NEXDKk8gZ\n",
    "    \"\"\"\n",
    "    steps = timesteps + 1\n",
    "    t = torch.linspace(0, timesteps, steps, dtype = torch.float64) / timesteps\n",
    "    alphas_cumprod = torch.cos((t + s) / (1 + s) * math.pi * 0.5) ** 2\n",
    "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "    return torch.clip(betas, 0, 0.999)\n",
    "\n",
    "def sigmoid_beta_schedule(timesteps, start = -3, end = 3, tau = 1, clamp_min = 1e-5):\n",
    "    \"\"\"\n",
    "    sigmoid schedule\n",
    "    proposed in https://arxiv.org/abs/2212.11972 - Figure 8\n",
    "    better for images > 64x64, when used during training\n",
    "    \"\"\"\n",
    "    steps = timesteps + 1\n",
    "    t = torch.linspace(0, timesteps, steps, dtype = torch.float64) / timesteps\n",
    "    v_start = torch.tensor(start / tau).sigmoid()\n",
    "    v_end = torch.tensor(end / tau).sigmoid()\n",
    "    alphas_cumprod = (-((t * (end - start) + start) / tau).sigmoid() + v_end) / (v_end - v_start)\n",
    "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "    return torch.clip(betas, 0, 0.999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main gaussian diffusion class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianDiffusion(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            model,\n",
    "            *,\n",
    "            image_size,\n",
    "            timesteps=1000,\n",
    "            sampling_timesteps=None,\n",
    "            objective='pred_noise',\n",
    "            beta_schedule='cosine',\n",
    "            schedule_fn_kwargs=dict(),\n",
    "            ddim_sampling_eta=0.,\n",
    "            auto_normalize=True,\n",
    "            offset_noise_strength=0.,  # https://www.crosslabs.org/blog/diffusion-with-offset-noise\n",
    "            min_snr_loss_weight=False,  # https://arxiv.org/abs/2303.09556\n",
    "            min_snr_gamma=5\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert not hasattr(model, 'random_or_learned_sinusoidal_cond') or not model.random_or_learned_sinusoidal_cond\n",
    "\n",
    "        self.model = model\n",
    "        self.self_condition = None\n",
    "\n",
    "        if isinstance(image_size, int):\n",
    "            image_size = (image_size, image_size)\n",
    "        assert isinstance(image_size, (tuple, list)) and len(\n",
    "            image_size) == 2, 'image size must be a integer or a tuple/list of two integers'\n",
    "        self.image_size = image_size\n",
    "\n",
    "        self.objective = objective\n",
    "\n",
    "        assert objective in {'pred_noise', 'pred_x0', 'pred_v'}, \\\n",
    "            'objective must be either pred_noise (predict noise) or pred_x0 (predict image start) or pred_v (predict v [v-parameterization as defined in appendix D of progressive distillation paper, used in imagen-video successfully])'\n",
    "\n",
    "        if beta_schedule == 'linear':\n",
    "            beta_schedule_fn = linear_beta_schedule\n",
    "        elif beta_schedule == 'cosine':\n",
    "            beta_schedule_fn = cosine_beta_schedule\n",
    "        elif beta_schedule == 'sigmoid':\n",
    "            beta_schedule_fn = sigmoid_beta_schedule\n",
    "        else:\n",
    "            raise ValueError(f'unknown beta schedule {beta_schedule}')\n",
    "\n",
    "        betas = beta_schedule_fn(timesteps, **schedule_fn_kwargs)\n",
    "\n",
    "        alphas = 1. - betas\n",
    "        alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "        alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.)\n",
    "\n",
    "        timesteps, = betas.shape\n",
    "        self.num_timesteps = int(timesteps)\n",
    "\n",
    "        # sampling related parameters\n",
    "\n",
    "        self.sampling_timesteps = default(sampling_timesteps,\n",
    "                                          timesteps)  # default num sampling timesteps to number of timesteps at training\n",
    "\n",
    "        assert self.sampling_timesteps <= timesteps\n",
    "        self.is_ddim_sampling = self.sampling_timesteps < timesteps\n",
    "        self.ddim_sampling_eta = ddim_sampling_eta\n",
    "\n",
    "        # helper function to register buffer from float64 to float32\n",
    "\n",
    "        register_buffer = lambda name, val: self.register_buffer(name, val.to(torch.float32))\n",
    "\n",
    "        register_buffer('betas', betas)\n",
    "        register_buffer('alphas_cumprod', alphas_cumprod)\n",
    "        register_buffer('alphas_cumprod_prev', alphas_cumprod_prev)\n",
    "\n",
    "        # calculations for diffusion q(x_t | x_{t-1}) and others\n",
    "\n",
    "        register_buffer('sqrt_alphas_cumprod', torch.sqrt(alphas_cumprod))\n",
    "        register_buffer('sqrt_one_minus_alphas_cumprod', torch.sqrt(1. - alphas_cumprod))\n",
    "        register_buffer('log_one_minus_alphas_cumprod', torch.log(1. - alphas_cumprod))\n",
    "        register_buffer('sqrt_recip_alphas_cumprod', torch.sqrt(1. / alphas_cumprod))\n",
    "        register_buffer('sqrt_recipm1_alphas_cumprod', torch.sqrt(1. / alphas_cumprod - 1))\n",
    "\n",
    "        # calculations for posterior q(x_{t-1} | x_t, x_0)\n",
    "\n",
    "        posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)\n",
    "\n",
    "        # above: equal to 1. / (1. / (1. - alpha_cumprod_tm1) + alpha_t / beta_t)\n",
    "\n",
    "        register_buffer('posterior_variance', posterior_variance)\n",
    "\n",
    "        # below: log calculation clipped because the posterior variance is 0 at the beginning of the diffusion chain\n",
    "\n",
    "        register_buffer('posterior_log_variance_clipped', torch.log(posterior_variance.clamp(min=1e-20)))\n",
    "        register_buffer('posterior_mean_coef1', betas * torch.sqrt(alphas_cumprod_prev) / (1. - alphas_cumprod))\n",
    "        register_buffer('posterior_mean_coef2', (1. - alphas_cumprod_prev) * torch.sqrt(alphas) / (1. - alphas_cumprod))\n",
    "\n",
    "        # offset noise strength - in blogpost, they claimed 0.1 was ideal\n",
    "\n",
    "        self.offset_noise_strength = offset_noise_strength\n",
    "\n",
    "        # derive loss weight\n",
    "        # snr - signal noise ratio\n",
    "\n",
    "        snr = alphas_cumprod / (1 - alphas_cumprod)\n",
    "\n",
    "        # https://arxiv.org/abs/2303.09556\n",
    "\n",
    "        maybe_clipped_snr = snr.clone()\n",
    "        if min_snr_loss_weight:\n",
    "            maybe_clipped_snr.clamp_(max=min_snr_gamma)\n",
    "\n",
    "        if objective == 'pred_noise':\n",
    "            register_buffer('loss_weight', maybe_clipped_snr / snr)\n",
    "        elif objective == 'pred_x0':\n",
    "            register_buffer('loss_weight', maybe_clipped_snr)\n",
    "        elif objective == 'pred_v':\n",
    "            register_buffer('loss_weight', maybe_clipped_snr / (snr + 1))\n",
    "\n",
    "        # auto-normalization of data [0, 1] -> [-1, 1] - can turn off by setting it to be False\n",
    "\n",
    "        self.normalize = normalize_to_neg_one_to_one if auto_normalize else identity\n",
    "        self.unnormalize = unnormalize_to_zero_to_one if auto_normalize else identity\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return self.betas.device\n",
    "\n",
    "    def predict_start_from_noise(self, x_t, t, noise):\n",
    "        if isinstance(noise, torch.Tensor):\n",
    "            noise[:, 1:, :, :] = 0\n",
    "        return (\n",
    "                extract(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t -\n",
    "                extract(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * noise\n",
    "        )\n",
    "\n",
    "    def predict_noise_from_start(self, x_t, t, x0):\n",
    "        return (\n",
    "                (extract(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t - x0) / \\\n",
    "                extract(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape)\n",
    "        )\n",
    "\n",
    "    def predict_v(self, x_start, t, noise):\n",
    "        return (\n",
    "                extract(self.sqrt_alphas_cumprod, t, x_start.shape) * noise -\n",
    "                extract(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * x_start\n",
    "        )\n",
    "\n",
    "    def predict_start_from_v(self, x_t, t, v):\n",
    "        return (\n",
    "                extract(self.sqrt_alphas_cumprod, t, x_t.shape) * x_t -\n",
    "                extract(self.sqrt_one_minus_alphas_cumprod, t, x_t.shape) * v\n",
    "        )\n",
    "\n",
    "    def q_posterior(self, x_start, x_t, t):\n",
    "        posterior_mean = (\n",
    "                extract(self.posterior_mean_coef1, t, x_t.shape) * x_start +\n",
    "                extract(self.posterior_mean_coef2, t, x_t.shape) * x_t\n",
    "        )\n",
    "        posterior_variance = extract(self.posterior_variance, t, x_t.shape)\n",
    "        posterior_log_variance_clipped = extract(self.posterior_log_variance_clipped, t, x_t.shape)\n",
    "        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n",
    "\n",
    "    def model_predictions(self, x, t, clip_x_start=False, rederive_pred_noise=False):\n",
    "        model_output = self.model(x, t)\n",
    "        zeros = torch.zeros_like(x, device=model_output.device)\n",
    "        zeros[:, 0, :, :] = model_output\n",
    "        model_output = zeros\n",
    "        maybe_clip = partial(torch.clamp, min=-1., max=1.) if clip_x_start else identity\n",
    "\n",
    "        if self.objective == 'pred_noise':\n",
    "            pred_noise = model_output\n",
    "            x_start = self.predict_start_from_noise(x, t, pred_noise)\n",
    "            x_start = maybe_clip(x_start)\n",
    "\n",
    "            if clip_x_start and rederive_pred_noise:\n",
    "                pred_noise = self.predict_noise_from_start(x, t, x_start)\n",
    "\n",
    "        elif self.objective == 'pred_x0':\n",
    "            x_start = model_output\n",
    "            x_start = maybe_clip(x_start)\n",
    "            pred_noise = self.predict_noise_from_start(x, t, x_start)\n",
    "\n",
    "        elif self.objective == 'pred_v':\n",
    "            v = model_output\n",
    "            x_start = self.predict_start_from_v(x, t, v)\n",
    "            x_start = maybe_clip(x_start)\n",
    "            pred_noise = self.predict_noise_from_start(x, t, x_start)\n",
    "\n",
    "        return ModelPrediction(pred_noise, x_start)\n",
    "\n",
    "    def p_mean_variance(self, x, t, x_self_cond=None, clip_denoised=True):\n",
    "        preds = self.model_predictions(x, t, x_self_cond)\n",
    "        x_start = preds.pred_x_start\n",
    "\n",
    "        if clip_denoised:\n",
    "            x_start.clamp_(-1., 1.)\n",
    "\n",
    "        model_mean, posterior_variance, posterior_log_variance = self.q_posterior(x_start=x_start, x_t=x, t=t)\n",
    "        return model_mean, posterior_variance, posterior_log_variance, x_start\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def p_sample(self, x, t: int, self_cond=None):\n",
    "        b, *_, device = *x.shape, self.device\n",
    "        batched_times = torch.full((b,), t, device=device, dtype=torch.long)\n",
    "        model_mean, _, model_log_variance, x_start = self.p_mean_variance(x=x, t=batched_times, x_self_cond=self_cond,\n",
    "                                                                          clip_denoised=True)\n",
    "        noise = torch.randn_like(x) if t > 0 else 0.  # no noise if t == 0\n",
    "        if isinstance(noise, torch.Tensor):\n",
    "            noise[:, 1:, :, :] = 0\n",
    "        pred_img = model_mean + (0.5 * model_log_variance).exp() * noise\n",
    "        return pred_img, x_start\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def p_sample_loop(self, x_start, return_all_timesteps=False):\n",
    "        batch, device = x_start.shape[0], self.device\n",
    "\n",
    "        img = x_start\n",
    "        imgs = [img]\n",
    "\n",
    "        x_start = None\n",
    "\n",
    "        for t in tqdm(reversed(range(0, self.num_timesteps)), desc='sampling loop time step', total=self.num_timesteps):\n",
    "            img, _ = self.p_sample(img, t, self_cond=None)\n",
    "            imgs.append(img)\n",
    "\n",
    "        ret = img if not return_all_timesteps else torch.stack(imgs, dim=1)\n",
    "\n",
    "        ret = self.unnormalize(ret)\n",
    "        return ret\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def ddim_sample(self, x_start, return_all_timesteps=False):\n",
    "        batch, device, total_timesteps, sampling_timesteps, eta, objective = x_start.shape[\n",
    "            0], self.device, self.num_timesteps, self.sampling_timesteps, self.ddim_sampling_eta, self.objective\n",
    "\n",
    "        times = torch.linspace(-1, total_timesteps - 1,\n",
    "                               steps=sampling_timesteps + 1)  # [-1, 0, 1, 2, ..., T-1] when sampling_timesteps == total_timesteps\n",
    "        times = list(reversed(times.int().tolist()))\n",
    "        time_pairs = list(zip(times[:-1], times[1:]))  # [(T-1, T-2), (T-2, T-3), ..., (1, 0), (0, -1)]\n",
    "\n",
    "        img = x_start\n",
    "        imgs = [img]\n",
    "\n",
    "        for time, time_next in tqdm(time_pairs, desc='sampling loop time step'):\n",
    "            time_cond = torch.full((batch,), time, device=device, dtype=torch.long)\n",
    "            pred_noise, _, *_ = self.model_predictions(img, time_cond, clip_x_start=True, rederive_pred_noise=True)\n",
    "\n",
    "            if time_next < 0:\n",
    "                img = x_start\n",
    "                imgs.append(img)\n",
    "                continue\n",
    "\n",
    "            alpha = self.alphas_cumprod[time]\n",
    "            alpha_next = self.alphas_cumprod[time_next]\n",
    "\n",
    "            sigma = eta * ((1 - alpha / alpha_next) * (1 - alpha_next) / (1 - alpha)).sqrt()\n",
    "            c = (1 - alpha_next - sigma ** 2).sqrt()\n",
    "\n",
    "            noise = torch.randn_like(img)\n",
    "            noise[:, 1:, :, :] = 0\n",
    "\n",
    "            img = x_start * alpha_next.sqrt() + \\\n",
    "                  c * pred_noise + \\\n",
    "                  sigma * noise\n",
    "\n",
    "            imgs.append(img)\n",
    "\n",
    "        ret = img if not return_all_timesteps else torch.stack(imgs, dim=1)\n",
    "\n",
    "        ret = self.unnormalize(ret)\n",
    "        return ret\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def sample(self, x_start, return_all_timesteps=False):\n",
    "        sample_fn = self.p_sample_loop if not self.is_ddim_sampling else self.ddim_sample\n",
    "        return sample_fn(x_start, return_all_timesteps=return_all_timesteps)\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def interpolate(self, x1, x2, t=None, lam=0.5):\n",
    "        b, *_, device = *x1.shape, x1.device\n",
    "        t = default(t, self.num_timesteps - 1)\n",
    "\n",
    "        assert x1.shape == x2.shape\n",
    "\n",
    "        t_batched = torch.full((b,), t, device=device)\n",
    "        xt1, xt2 = map(lambda x: self.q_sample(x, t=t_batched), (x1, x2))\n",
    "\n",
    "        img = (1 - lam) * xt1 + lam * xt2\n",
    "\n",
    "        x_start = None\n",
    "\n",
    "        for i in tqdm(reversed(range(0, t)), desc='interpolation sample time step', total=t):\n",
    "            self_cond = x_start if self.self_condition else None\n",
    "            img, x_start = self.p_sample(img, i, self_cond)\n",
    "\n",
    "        return img\n",
    "\n",
    "    @autocast(enabled=False)\n",
    "    def q_sample(self, x_start, t, noise=None):\n",
    "        b, c, h, w = x_start.shape\n",
    "        noise = default(noise, lambda: torch.randn_like(x_start))\n",
    "        noise[:, 1:, :, :] = 0\n",
    "\n",
    "        return (\n",
    "                extract(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start +\n",
    "                extract(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * noise\n",
    "        )\n",
    "\n",
    "    def p_losses(self, x_start, t, noise=None, offset_noise_strength=None):\n",
    "        b, c, h, w = x_start.shape\n",
    "\n",
    "        noise = default(noise, lambda: torch.randn_like(x_start))\n",
    "\n",
    "        # offset noise - https://www.crosslabs.org/blog/diffusion-with-offset-noise\n",
    "\n",
    "        offset_noise_strength = default(offset_noise_strength, self.offset_noise_strength)\n",
    "\n",
    "        if offset_noise_strength > 0.:\n",
    "            offset_noise = torch.randn(x_start.shape[:2], device=self.device)\n",
    "            noise += offset_noise_strength * rearrange(offset_noise, 'b c -> b c 1 1')\n",
    "\n",
    "        # noise sample\n",
    "        mask = x_start[:,0,:,:] == -10\n",
    "        x_start[:,0,:,:][mask] = 0\n",
    "        x = self.q_sample(x_start=x_start, t=t, noise=noise)\n",
    "\n",
    "        # if doing self-conditioning, 50% of the time, predict x_start from current set of times\n",
    "        # and condition with unet with that\n",
    "        # this technique will slow down training by 25%, but seems to lower FID significantly\n",
    "\n",
    "        # predict and take gradient step\n",
    "        # print(x.shape, t.shape)\n",
    "        model_out = self.model(x, t)\n",
    "        model_out[mask] = 0\n",
    "\n",
    "        if self.objective == 'pred_noise':\n",
    "            target = noise[:, 0, :, :]\n",
    "        elif self.objective == 'pred_x0':\n",
    "            target = x_start\n",
    "        elif self.objective == 'pred_v':\n",
    "            v = self.predict_v(x_start, t, noise)\n",
    "            target = v\n",
    "        else:\n",
    "            raise ValueError(f'unknown objective {self.objective}')\n",
    "        \n",
    "        loss = F.mse_loss(model_out, target, reduction='none')\n",
    "        loss = reduce(loss, 'b ... -> b', 'mean')\n",
    "\n",
    "        loss = loss * extract(self.loss_weight, t, loss.shape)\n",
    "        return loss.mean()\n",
    "\n",
    "    def forward(self, img, *args, **kwargs):\n",
    "        b, c, h, w, device, img_size, = *img.shape, img.device, self.image_size\n",
    "        assert h == img_size[0] and w == img_size[1], f'height and width of image must be {img_size}'\n",
    "        t = torch.randint(0, self.num_timesteps, (b,), device=device).long()\n",
    "\n",
    "        return self.p_losses(img, t, *args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(\n",
    "            self,\n",
    "            diffusion_model,\n",
    "            folder,\n",
    "            *,\n",
    "            train_batch_size=16,\n",
    "            gradient_accumulate_every=1,\n",
    "            train_lr=1e-4,\n",
    "            train_num_steps=100000,\n",
    "            ema_update_every=10,\n",
    "            ema_decay=0.995,\n",
    "            adam_betas=(0.9, 0.99),\n",
    "            save_and_sample_every=1000,\n",
    "            num_samples=25,\n",
    "            n_subsamples=1600,\n",
    "            min_edges_per_subsample=10,\n",
    "            results_folder='./results',\n",
    "            amp=False,\n",
    "            mixed_precision_type='fp16',\n",
    "            split_batches=True,\n",
    "            inception_block_idx=2048,\n",
    "            max_grad_norm=1.,\n",
    "            save_best_and_latest_only=False\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # accelerator\n",
    "\n",
    "        self.accelerator = Accelerator(\n",
    "            split_batches=split_batches,\n",
    "            mixed_precision=mixed_precision_type if amp else 'no'\n",
    "        )\n",
    "\n",
    "        # model\n",
    "\n",
    "        self.model = diffusion_model\n",
    "        is_ddim_sampling = diffusion_model.is_ddim_sampling\n",
    "\n",
    "        # sampling and training hyperparameters\n",
    "\n",
    "        assert has_int_squareroot(num_samples), 'number of samples must have an integer square root'\n",
    "        self.num_samples = num_samples\n",
    "        self.save_and_sample_every = save_and_sample_every\n",
    "\n",
    "        self.batch_size = train_batch_size\n",
    "        self.gradient_accumulate_every = gradient_accumulate_every\n",
    "        assert (train_batch_size * gradient_accumulate_every) >= 16, \\\n",
    "            f'your effective batch size (train_batch_size x gradient_accumulate_every) should be at least 16 or above'\n",
    "\n",
    "        self.train_num_steps = train_num_steps\n",
    "        self.image_size = diffusion_model.image_size\n",
    "\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "\n",
    "        # dataset and dataloader\n",
    "        self.ds = ProcessedMovieLens(folder, n_subsamples, n_unique_per_sample=min_edges_per_subsample,\n",
    "                                     dataset_transform=rating_transform, download=True)\n",
    "\n",
    "        dl = DataLoader(self.ds, batch_size=train_batch_size, shuffle=True, pin_memory=True, num_workers=cpu_count())\n",
    "\n",
    "        dl = self.accelerator.prepare(dl)\n",
    "        self.dl = cycle(dl)\n",
    "\n",
    "        # optimizer\n",
    "\n",
    "        self.opt = Adam(diffusion_model.parameters(), lr=train_lr, betas=adam_betas)\n",
    "\n",
    "        # for logging results in a folder periodically\n",
    "\n",
    "        if self.accelerator.is_main_process:\n",
    "            self.ema = EMA(diffusion_model, beta=ema_decay, update_every=ema_update_every)\n",
    "            self.ema.to(self.device)\n",
    "\n",
    "        self.results_folder = Path(results_folder)\n",
    "        self.results_folder.mkdir(exist_ok=True)\n",
    "\n",
    "        # step counter state\n",
    "\n",
    "        self.step = 0\n",
    "\n",
    "        # prepare model, dataloader, optimizer with accelerator\n",
    "\n",
    "        self.model, self.opt = self.accelerator.prepare(self.model, self.opt)\n",
    "\n",
    "        self.save_best_and_latest_only = save_best_and_latest_only\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return self.accelerator.device\n",
    "\n",
    "    def save(self, milestone):\n",
    "        if not self.accelerator.is_local_main_process:\n",
    "            return\n",
    "\n",
    "        data = {\n",
    "            'step': self.step,\n",
    "            'model': self.accelerator.get_state_dict(self.model),\n",
    "            'opt': self.opt.state_dict(),\n",
    "            'ema': self.ema.state_dict(),\n",
    "            'scaler': self.accelerator.scaler.state_dict() if exists(self.accelerator.scaler) else None\n",
    "        }\n",
    "\n",
    "        torch.save(data, str(self.results_folder / f'model-{milestone}.pt'))\n",
    "\n",
    "    def load(self, milestone):\n",
    "        accelerator = self.accelerator\n",
    "        device = accelerator.device\n",
    "\n",
    "        data = torch.load(str(self.results_folder / f'model-{milestone}.pt'), map_location=device)\n",
    "\n",
    "        model = self.accelerator.unwrap_model(self.model)\n",
    "        model.load_state_dict(data['model'])\n",
    "\n",
    "        self.step = data['step']\n",
    "        self.opt.load_state_dict(data['opt'])\n",
    "        if self.accelerator.is_main_process:\n",
    "            self.ema.load_state_dict(data[\"ema\"])\n",
    "\n",
    "        if 'version' in data:\n",
    "            print(f\"loading from version {data['version']}\")\n",
    "\n",
    "        if exists(self.accelerator.scaler) and exists(data['scaler']):\n",
    "            self.accelerator.scaler.load_state_dict(data['scaler'])\n",
    "\n",
    "    def train(self):\n",
    "        accelerator = self.accelerator\n",
    "        device = accelerator.device\n",
    "\n",
    "        with tqdm(initial=self.step, total=self.train_num_steps, disable=not accelerator.is_main_process) as pbar:\n",
    "\n",
    "            while self.step < self.train_num_steps:\n",
    "\n",
    "                total_loss = 0.\n",
    "\n",
    "                for _ in range(self.gradient_accumulate_every):\n",
    "                    data = next(self.dl).to(device)\n",
    "\n",
    "                    with self.accelerator.autocast():\n",
    "                        loss = self.model(data)\n",
    "                        loss = loss / self.gradient_accumulate_every\n",
    "                        total_loss += loss.item()\n",
    "\n",
    "                    self.accelerator.backward(loss)\n",
    "\n",
    "                pbar.set_description(f'loss: {total_loss:.4f}')\n",
    "\n",
    "                accelerator.wait_for_everyone()\n",
    "                accelerator.clip_grad_norm_(self.model.parameters(), self.max_grad_norm)\n",
    "\n",
    "                self.opt.step()\n",
    "                self.opt.zero_grad()\n",
    "\n",
    "                accelerator.wait_for_everyone()\n",
    "\n",
    "                self.step += 1\n",
    "                if accelerator.is_main_process:\n",
    "                    self.ema.update()\n",
    "\n",
    "                    if self.step != 0 and divisible_by(self.step, self.save_and_sample_every):\n",
    "                        self.ema.ema_model.eval()\n",
    "\n",
    "                        with torch.inference_mode():\n",
    "                            eval_data = next(self.dl).to(device)\n",
    "                            b, c, h, w = eval_data.shape\n",
    "                            random_rating = torch.randn(b, h, w)\n",
    "                            eval_data[:, 0, :, :] = random_rating\n",
    "\n",
    "                            val_loss = self.model(eval_data)\n",
    "                            val_sample = self.ema.ema_model.sample(eval_data)\n",
    "                            print(f\"Validation Loss: {val_loss.item()}\")\n",
    "                            np.save(\n",
    "                                f\"{self.results_folder}/sample-{self.step}.npy\",\n",
    "                                val_sample[:, 0, :, :].cpu().detach().numpy()\n",
    "                            )\n",
    "                            self.save(self.step)\n",
    "\n",
    "                pbar.update(1)\n",
    "\n",
    "        accelerator.print('training complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MOVIELENS DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MOVIE_HEADERS = [\"movieId\", \"title\", \"genres\"]\n",
    "USER_HEADERS = [\"userId\", \"gender\", \"age\", \"occupation\", \"zipCode\"]\n",
    "RATING_HEADERS = ['userId', 'movieId', 'rating', 'timestamp']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rating transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rating_transform(data):\n",
    "    ratings = data[2, :]\n",
    "    ratings[ratings == 1] = -5\n",
    "    ratings[ratings == 2] = -2\n",
    "    ratings[ratings == 3] = 0\n",
    "    ratings[ratings == 4] = 2\n",
    "    data[2, :] = ratings\n",
    "    return data\n",
    "\n",
    "class RatingQuantileTransform(object):\n",
    "    def __init__(self):\n",
    "        self.qt_transformer = QuantileTransformer(n_quantiles=5, output_distribution=\"normal\")\n",
    "\n",
    "    def __call__(self, data):\n",
    "        ratings = data[2, :]\n",
    "        data[2, :] = torch.Tensor(self.qt_transformer.fit_transform(ratings.reshape(-1, 1))).T\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### self-processing dataset features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RawMovieLens1M(MovieLens1M):\n",
    "    def __init__(self, root, transform=None, pre_transform=None, force_reload=False):\n",
    "        super(RawMovieLens1M, self).__init__(root, transform, pre_transform, force_reload)\n",
    "\n",
    "    def _process_genres(self, df):\n",
    "        l = df[\"genres\"].str.get_dummies('|').values\n",
    "        max_genres = l.sum(axis=1).max()\n",
    "        idx_list = []\n",
    "        for i in range(l.shape[0]):\n",
    "            idxs = np.where(l[i, :] == 1)[0] + 1\n",
    "            missing = max_genres - len(idxs)\n",
    "            if missing > 0:\n",
    "                idxs = np.array(list(idxs) + missing * [0])\n",
    "            idx_list.append(idxs)\n",
    "        return np.stack(idx_list)\n",
    "\n",
    "    def process(self) -> None:\n",
    "        import pandas as pd\n",
    "\n",
    "        data = HeteroData()\n",
    "\n",
    "        # Process movie data:\n",
    "        df = pd.read_csv(\n",
    "            self.raw_paths[0],\n",
    "            sep='::',\n",
    "            header=None,\n",
    "            index_col='movieId',\n",
    "            names=MOVIE_HEADERS,\n",
    "            encoding='ISO-8859-1',\n",
    "            engine='python',\n",
    "        )\n",
    "        movie_mapping = {idx: i for i, idx in enumerate(df.index)}\n",
    "\n",
    "        genres = self._process_genres(df)\n",
    "        genres = torch.from_numpy(genres).to(torch.float)\n",
    "\n",
    "        data['movie'].x = genres\n",
    "\n",
    "        # Process user data:\n",
    "        df = pd.read_csv(\n",
    "            self.raw_paths[1],\n",
    "            sep='::',\n",
    "            header=None,\n",
    "            index_col='userId',\n",
    "            names=USER_HEADERS,\n",
    "            dtype='str',\n",
    "            encoding='ISO-8859-1',\n",
    "            engine='python',\n",
    "        )\n",
    "        user_mapping = {idx: i for i, idx in enumerate(df.index)}\n",
    "\n",
    "        age = df['age'].str.get_dummies().values.argmax(axis=1)[:, None]\n",
    "        age = torch.from_numpy(age).to(torch.float)\n",
    "\n",
    "        gender = df['gender'].str.get_dummies().values[:, 0][:, None]\n",
    "        gender = torch.from_numpy(gender).to(torch.float)\n",
    "\n",
    "        occupation = df['occupation'].str.get_dummies().values.argmax(axis=1)[:, None]\n",
    "        occupation = torch.from_numpy(occupation).to(torch.float)\n",
    "\n",
    "        data['user'].x = torch.cat([age, gender, occupation], dim=-1)\n",
    "\n",
    "        self.int_user_data = df\n",
    "\n",
    "        # Process rating data:\n",
    "        df = pd.read_csv(\n",
    "            self.raw_paths[2],\n",
    "            sep='::',\n",
    "            header=None,\n",
    "            names=RATING_HEADERS,\n",
    "            encoding='ISO-8859-1',\n",
    "            engine='python',\n",
    "        )\n",
    "\n",
    "        src = [user_mapping[idx] for idx in df['userId']]\n",
    "        dst = [movie_mapping[idx] for idx in df['movieId']]\n",
    "        edge_index = torch.tensor([src, dst])\n",
    "        data['user', 'rates', 'movie'].edge_index = edge_index\n",
    "\n",
    "        rating = torch.from_numpy(df['rating'].values).to(torch.long)\n",
    "        data['user', 'rates', 'movie'].rating = rating\n",
    "\n",
    "        time = torch.from_numpy(df['timestamp'].values)\n",
    "        data['user', 'rates', 'movie'].time = time\n",
    "\n",
    "        data['movie', 'rated_by', 'user'].edge_index = edge_index.flip([0])\n",
    "        data['movie', 'rated_by', 'user'].rating = rating\n",
    "        data['movie', 'rated_by', 'user'].time = time\n",
    "\n",
    "        if self.pre_transform is not None:\n",
    "            data = self.pre_transform(data)\n",
    "\n",
    "        self.save([data], self.processed_paths[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProcessedMovieLens(Dataset):\n",
    "    PROCESSED_ML_SUBPATH = \"/processed/data.pt\"\n",
    "\n",
    "    def __init__(self, root, n_subsamples=100, n_unique_per_sample=10, dataset_transform=None, transform=None,\n",
    "                 download=True):\n",
    "        if download:\n",
    "            self.ml_1m = RawMovieLens1M(root, force_reload=True)\n",
    "            self.ml_1m.process()\n",
    "\n",
    "        self.n_unique_per_sample = n_unique_per_sample\n",
    "        self.n_subsamples = n_subsamples\n",
    "        self.transform = transform\n",
    "        self.dataset_transform = dataset_transform\n",
    "        print(root + self.PROCESSED_ML_SUBPATH)\n",
    "        self.processed_data = torch.load(root + self.PROCESSED_ML_SUBPATH)\n",
    "        self.processed_ratings = self._preprocess_ratings(self.processed_data)\n",
    "\n",
    "    def _preprocess_ratings(self, data):\n",
    "        edges = data[0][('user', 'rates', 'movie')]\n",
    "        edge_ratings = torch.concatenate([edges[\"edge_index\"], edges[\"rating\"].reshape((1, -1))])\n",
    "        return self.dataset_transform(edge_ratings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        n_unique = self.n_unique_per_sample\n",
    "        edge_ratings = self.processed_ratings\n",
    "        movie_feats, user_feats = self.processed_data[0][\"movie\"][\"x\"], self.processed_data[0][\"user\"][\"x\"]\n",
    "\n",
    "        _, edge_size = edge_ratings.shape\n",
    "        indices = torch.randint(0, edge_size, (n_unique,))\n",
    "        sampled_edges = torch.ones((1, 1))\n",
    "        while len(sampled_edges[0, :].unique()) < n_unique or len(sampled_edges[1, :].unique()) < n_unique:\n",
    "            indices = torch.randint(0, edge_size, (n_unique,))\n",
    "            sampled_edges = edge_ratings[:, indices]\n",
    "\n",
    "        xs = edge_ratings[0, indices]\n",
    "        ys = edge_ratings[1, indices]\n",
    "\n",
    "        indices_xs = torch.where(torch.isin(edge_ratings[0, :], xs))[0]\n",
    "        indices_ys = torch.where(torch.isin(edge_ratings[1, :], ys))[0]\n",
    "        subsample_edges = edge_ratings[:, np.intersect1d(indices_xs, indices_ys)].T\n",
    "\n",
    "        subsample_movie_feats = movie_feats[subsample_edges[:, 1], :]\n",
    "        subsample_user_feats = user_feats[subsample_edges[:, 0], :]\n",
    "        subsample_user_movie_feats = torch.cat([subsample_movie_feats, subsample_user_feats], dim=1)\n",
    "\n",
    "        broadcasted_movie_feats, broadcasted_user_feats = (\n",
    "            torch.broadcast_to(\n",
    "                movie_feats[ys.unique().sort().values, :].T.reshape((-1, 1, n_unique)).swapaxes(1, 2),\n",
    "                (-1, n_unique, n_unique)\n",
    "            ).swapaxes(1, 2),\n",
    "            torch.broadcast_to(\n",
    "                user_feats[xs.unique().sort().values, :].T.reshape((-1, 1, n_unique)).swapaxes(1, 2),\n",
    "                (-1, n_unique, n_unique)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        rating_matrix = torch.Tensor(pd.DataFrame(subsample_edges).pivot(columns=[1], index=[0]).fillna(-10).to_numpy())\n",
    "        item = torch.cat([\n",
    "            rating_matrix.reshape((1, n_unique, n_unique)),\n",
    "            broadcasted_movie_feats,\n",
    "            broadcasted_user_feats\n",
    "        ], dim=0)\n",
    "\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_subsamples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### feature embedding head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieLensFeatureEmb(nn.Module):\n",
    "    MAX_N_GENRES = 6\n",
    "    \n",
    "    def __init__(self,\n",
    "                 age_dim = 4,\n",
    "                 gender_dim = 3,\n",
    "                 occupation_dim = 8,\n",
    "                 genre_dim = 16,\n",
    "        ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.age_embedding = nn.Embedding(\n",
    "            num_embeddings = 6,\n",
    "            embedding_dim = age_dim\n",
    "        )\n",
    "        self.gender_embedding = nn.Embedding(\n",
    "            num_embeddings = 2,\n",
    "            embedding_dim = gender_dim\n",
    "        )\n",
    "        self.occupation_embedding = nn.Embedding(\n",
    "            num_embeddings = 21,\n",
    "            embedding_dim = occupation_dim\n",
    "        )\n",
    "        self.genre_embedding = nn.Embedding(\n",
    "            num_embeddings = 19,\n",
    "            embedding_dim = genre_dim\n",
    "        )\n",
    "        \n",
    "        self.age_dim = age_dim\n",
    "        self.gender_dim = gender_dim\n",
    "        self.occupation_dim = occupation_dim\n",
    "        self.genre_dim = genre_dim\n",
    "        \n",
    "    @property\n",
    "    def embed_dim(self):\n",
    "        return self.age_dim + self.gender_dim + self.occupation_dim + self.genre_dim + 1\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # dims = [ft, user, movie]\n",
    "        # ft = [1 rating, 6 genres, --> these are all bogus rn --> 1 age, 1 gender, 1 occupation]\n",
    "        # x.shape = (b, f, n, m)\n",
    "        assert x.shape[1] == 4 + self.MAX_N_GENRES\n",
    "\n",
    "        ratings = x[:, 0:1]\n",
    "        genres = x[:, 1:7].long()\n",
    "        age = x[:, 7].long()\n",
    "        gender = x[:, 8].long()\n",
    "        occupation = x[:, 9].long()\n",
    "        \n",
    "        genre_emb = self.genre_embedding(genres).swapdims(1, -1).sum(dim=-1)\n",
    "        age_emb = self.age_embedding(age).permute(0, 3, 1, 2)\n",
    "        gender_emb = self.gender_embedding(gender).permute(0, 3, 1, 2)\n",
    "        occupation_emb = self.occupation_embedding(occupation).permute(0, 3, 1, 2)\n",
    "\n",
    "        full_embeds = torch.cat([ratings, genre_emb, age_emb, gender_emb, occupation_emb], dim=1)\n",
    "        assert full_embeds.shape[1] == self.embed_dim\n",
    "\n",
    "        return full_embeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DENOISING MODEL (SUBGRAPH DIT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelPrediction =  namedtuple('ModelPrediction', ['pred_noise', 'pred_x_start'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### small helper modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        return self.fn(x, *args, **kwargs) + x\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.g = nn.Parameter(torch.ones(1, dim, 1, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.normalize(x, dim=1) * self.g * (x.shape[1] ** 0.5)\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.norm = RMSNorm(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        return self.fn(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sinusoidal positional embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalPosEmb(nn.Module):\n",
    "    def __init__(self, dim, theta = 10000):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.theta = theta\n",
    "\n",
    "    def forward(self, x):\n",
    "        device = x.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb = math.log(self.theta) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
    "        emb = x[:, None] * emb[None, :]\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "        return emb\n",
    "\n",
    "class RandomOrLearnedSinusoidalPosEmb(nn.Module):\n",
    "    \"\"\" following @crowsonkb 's lead with random (learned optional) sinusoidal pos emb \"\"\"\n",
    "    \"\"\" https://github.com/crowsonkb/v-diffusion-jax/blob/master/diffusion/models/danbooru_128.py#L8 \"\"\"\n",
    "\n",
    "    def __init__(self, dim, is_random = False):\n",
    "        super().__init__()\n",
    "        assert divisible_by(dim, 2)\n",
    "        half_dim = dim // 2\n",
    "        self.dim = dim + 1\n",
    "        self.weights = nn.Parameter(torch.randn(half_dim), requires_grad = not is_random)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = rearrange(x, 'b -> b 1')\n",
    "        freqs = x * rearrange(self.weights, 'd -> 1 d') * 2 * math.pi\n",
    "        fouriered = torch.cat((freqs.sin(), freqs.cos()), dim = -1)\n",
    "        fouriered = torch.cat((x, fouriered), dim = -1)\n",
    "        return fouriered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sugraph DiT block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubgraphDiTBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_dim,\n",
    "        out_dim,\n",
    "        ffn_hidden_dims = None,\n",
    "        ffn_activation = nn.SiLU(),\n",
    "        attn_heads = 4,\n",
    "        attn_dim_head = 32,\n",
    "        time_emb_dim = None,\n",
    "        flash = False\n",
    "\t):\n",
    "        super().__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        \n",
    "        # mlps for timestep conditioning using adaptive layer norm\n",
    "        self.time_mlp_pre = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_emb_dim, in_dim*8)\n",
    "        ) if exists(time_emb_dim) else None\n",
    "        \n",
    "        self.time_mlp_post = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_emb_dim, in_dim*2)\n",
    "        ) if exists(time_emb_dim) else None\n",
    "        \n",
    "        # layer norm for input\n",
    "        self.input_ln = RMSNorm(in_dim)\n",
    "        \n",
    "        # row and column attention\n",
    "        self.row_attn = RowAttention(in_dim, attn_heads, attn_dim_head, flash)\n",
    "        self.col_attn = RowAttention(in_dim, attn_heads, attn_dim_head, flash)\n",
    "        \n",
    "        # post residual layer norm\n",
    "        self.concat_ln = RMSNorm(in_dim*2)\n",
    "        \n",
    "        # pointwise feedforward network for attention + residual\n",
    "        if exists(ffn_hidden_dims):\n",
    "            ffn_in_dims = [in_dim*2] + ffn_hidden_dims[:-1]\n",
    "            ffn_out_dims = ffn_hidden_dims\n",
    "            self.ffn = nn.Sequential(\n",
    "                *[\n",
    "                    nn.Sequential(\n",
    "                        nn.Conv2d(dim_in, dim_out, 1),\n",
    "                        ffn_activation\n",
    "                    ) for dim_in, dim_out in zip(ffn_in_dims, ffn_out_dims)\n",
    "                 ],\n",
    "                nn.Conv2d(ffn_in_dims[-1], in_dim*2, 1)\n",
    "            )\n",
    "        else:\n",
    "            self.ffn = nn.Conv2d(in_dim*2, in_dim*2, 1)\n",
    "        \n",
    "        # final downsample for ffn output + residual\n",
    "        self.downsample = nn.Conv2d(in_dim*2, out_dim, 1)\n",
    "        \n",
    "    def forward(self, x, time_emb = None):\n",
    "        input_residual = x   \n",
    "        \n",
    "        if exists(self.time_mlp_pre) and exists(time_emb):\n",
    "            time_emb_pre = self.time_mlp_pre(time_emb)\n",
    "            time_emb_pre = rearrange(time_emb_pre, 'b c -> b c 1 1')\n",
    "            \n",
    "            pre_attn_scale, pre_attn_shift, post_row_attn_scale, post_col_attn_scale, pre_ffn_scale1, pre_ffn_scale2, pre_ffn_shift1, pre_ffn_shift2 = time_emb_pre.chunk(8, dim = 1)\n",
    "            \n",
    "            pre_ffn_scale = torch.cat([pre_ffn_scale1, pre_ffn_scale2], dim = 1)\n",
    "            pre_ffn_shift = torch.cat([pre_ffn_shift1, pre_ffn_shift2], dim = 1)\n",
    "            \n",
    "            post_ffn_scale = self.time_mlp_post(time_emb)\n",
    "            post_ffn_scale = rearrange(post_ffn_scale, 'b c -> b c 1 1')\n",
    "        \n",
    "        x = self.input_ln(x)\n",
    "        x = x*(1 + pre_attn_scale) + pre_attn_shift\n",
    "        \n",
    "        row_attn = self.row_attn(x)\n",
    "        row_attn = row_attn*(1 + post_row_attn_scale)\n",
    "        \n",
    "        col_attn = self.col_attn(x.transpose(-2, -1)).transpose(-2, -1)\n",
    "        col_attn = col_attn*(1 + post_col_attn_scale)\n",
    "        \n",
    "        row_attn = row_attn + input_residual*0.5\n",
    "        col_attn = col_attn + input_residual*0.5\n",
    "        concatenated = torch.cat([row_attn, col_attn], dim=1)\n",
    "        \n",
    "        x = self.concat_ln(concatenated)\n",
    "        x = x*(1 + pre_ffn_scale) + pre_ffn_shift\n",
    "        x = self.ffn(x)\n",
    "        x = x*(1 + post_ffn_scale)\n",
    "        \n",
    "        x = x + concatenated\n",
    "        out = self.downsample(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### subgraph DiT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubgraphDiT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_dims = None,\n",
    "        out_dim = 1,\n",
    "        feature_embedding = MovieLensFeatureEmb(),\n",
    "        time_embedding = RandomOrLearnedSinusoidalPosEmb,\n",
    "        ffn_hidden_dims = None,\n",
    "        ffn_activation = nn.SiLU(),\n",
    "        attn_heads = 4,\n",
    "        attn_dim_head = 32,\n",
    "        final_ffn_hidden_dims = None,\n",
    "        final_ffn_activation = nn.SiLU(),\n",
    "        final_attn_heads = 4,\n",
    "        final_attn_dim_head = 32,\n",
    "        flash = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.feature_dim = feature_embedding.embed_dim\n",
    "        self.feature_embedder = feature_embedding\n",
    "        \n",
    "        self.time_embedding = time_embedding(self.feature_dim)\n",
    "        self.time_embedder = nn.Sequential(\n",
    "            self.time_embedding,\n",
    "            nn.Linear(self.time_embedding.dim, 4*self.feature_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(4*self.feature_dim, 4*self.feature_dim)\n",
    "        )\n",
    "        self.blocks = nn.ModuleList([])\n",
    "        \n",
    "        mid_dim = self.feature_dim\n",
    "        for i, hidden_dim in enumerate(hidden_dims):\n",
    "            self.blocks.append(SubgraphDiTBlock(\n",
    "                in_dim = mid_dim,\n",
    "                out_dim = hidden_dim,\n",
    "                ffn_hidden_dims = ffn_hidden_dims[i] if exists(ffn_hidden_dims) else None,\n",
    "                ffn_activation = ffn_activation[i] if isinstance(ffn_activation, list) else ffn_activation,\n",
    "                attn_heads = attn_heads[i] if isinstance(attn_heads, list) else attn_heads,\n",
    "                attn_dim_head = attn_dim_head[i] if isinstance(attn_dim_head, list) else attn_dim_head,\n",
    "                time_emb_dim = 4*self.feature_dim,\n",
    "                flash = flash\n",
    "            ))\n",
    "            mid_dim = hidden_dim\n",
    "            \n",
    "        self.blocks.append(SubgraphDiTBlock(\n",
    "            in_dim = mid_dim,\n",
    "            out_dim = out_dim,\n",
    "            ffn_hidden_dims = final_ffn_hidden_dims,\n",
    "            ffn_activation = final_ffn_activation,\n",
    "            attn_heads = final_attn_heads,\n",
    "            attn_dim_head = final_attn_dim_head,\n",
    "            time_emb_dim = 4*self.feature_dim,\n",
    "            flash = flash\n",
    "        ))\n",
    "            \n",
    "        self.layer_norm = RMSNorm(out_dim)\n",
    "        self.to_out = nn.Conv2d(out_dim, out_dim, 1)\n",
    "        \n",
    "    def forward(self, x, time):\n",
    "        time_emb = self.time_embedder(time)\n",
    "        x = self.feature_embedder(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x, time_emb)\n",
    "            x = self.layer_norm(x)\n",
    "        return self.to_out(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SubgraphDiT(\n",
    "    hidden_dims=[16, 8, 4],\n",
    "    ffn_hidden_dims=[[32, 32], [16, 16], [8, 8]],\n",
    "    ffn_activation=[nn.SiLU(), nn.SiLU(), nn.SiLU()],\n",
    "    attn_heads=[4, 4, 4],\n",
    "    attn_dim_head=[32, 16, 8],\n",
    "    final_ffn_hidden_dims=[4, 4],\n",
    "    final_ffn_activation=nn.SiLU(),\n",
    "    final_attn_heads=4,\n",
    "    final_attn_dim_head=8,\n",
    "    flash=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXECUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hemit/anaconda3/envs/edge/lib/python3.11/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(split_batches=True)\n",
      "  warnings.warn(\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./movie_lens/processed/data.pt\n"
     ]
    }
   ],
   "source": [
    "diffusion = GaussianDiffusion(model, image_size=10)\n",
    "trainer = Trainer(diffusion, \"./movie_lens\", train_num_steps=int(1e4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:01<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape torch.Size([16, 10, 10, 10])\n",
      "Time Shape torch.Size([16])\n",
      "Time Emb Shape torch.Size([16, 128])\n",
      "Feature Embed Shape torch.Size([16, 32, 10, 10])\n",
      "Feature map shape:  torch.Size([16, 32, 10, 10])\n",
      "Feature map shape after input layer norm:  torch.Size([16, 32, 10, 10])\n",
      "Shape after pre_attn modulate:  torch.Size([16, 32, 10, 10])\n",
      "Shape after row attention:  torch.Size([16, 32, 10, 10])\n",
      "Shape after post row attention:  torch.Size([16, 32, 10, 10])\n",
      "Shape after col attention:  torch.Size([16, 32, 10, 10])\n",
      "Shape after post col attention:  torch.Size([16, 32, 10, 10])\n",
      "Shape after row attn residual:  torch.Size([16, 32, 10, 10])\n",
      "Shape after col attn residual:  torch.Size([16, 32, 10, 10])\n",
      "Shape after concatenation:  torch.Size([16, 64, 10, 10])\n",
      "Shape after concat layer norm:  torch.Size([16, 64, 10, 10])\n",
      "Shape after pre_ffn modulate:  torch.Size([16, 64, 10, 10])\n",
      "Shape after ffn:  torch.Size([16, 64, 10, 10])\n",
      "Shape after post ffn modulate:  torch.Size([16, 64, 10, 10])\n",
      "Shape after residual connection:  torch.Size([16, 64, 10, 10])\n",
      "Shape after downsample:  torch.Size([16, 16, 10, 10])\n",
      "Feature map shape:  torch.Size([16, 16, 10, 10])\n",
      "Feature map shape after input layer norm:  torch.Size([16, 16, 10, 10])\n",
      "Shape after pre_attn modulate:  torch.Size([16, 16, 10, 10])\n",
      "Shape after row attention:  torch.Size([16, 16, 10, 10])\n",
      "Shape after post row attention:  torch.Size([16, 16, 10, 10])\n",
      "Shape after col attention:  torch.Size([16, 16, 10, 10])\n",
      "Shape after post col attention:  torch.Size([16, 16, 10, 10])\n",
      "Shape after row attn residual:  torch.Size([16, 16, 10, 10])\n",
      "Shape after col attn residual:  torch.Size([16, 16, 10, 10])\n",
      "Shape after concatenation:  torch.Size([16, 32, 10, 10])\n",
      "Shape after concat layer norm:  torch.Size([16, 32, 10, 10])\n",
      "Shape after pre_ffn modulate:  torch.Size([16, 32, 10, 10])\n",
      "Shape after ffn:  torch.Size([16, 32, 10, 10])\n",
      "Shape after post ffn modulate:  torch.Size([16, 32, 10, 10])\n",
      "Shape after residual connection:  torch.Size([16, 32, 10, 10])\n",
      "Shape after downsample:  torch.Size([16, 8, 10, 10])\n",
      "Feature map shape:  torch.Size([16, 8, 10, 10])\n",
      "Feature map shape after input layer norm:  torch.Size([16, 8, 10, 10])\n",
      "Shape after pre_attn modulate:  torch.Size([16, 8, 10, 10])\n",
      "Shape after row attention:  torch.Size([16, 8, 10, 10])\n",
      "Shape after post row attention:  torch.Size([16, 8, 10, 10])\n",
      "Shape after col attention:  torch.Size([16, 8, 10, 10])\n",
      "Shape after post col attention:  torch.Size([16, 8, 10, 10])\n",
      "Shape after row attn residual:  torch.Size([16, 8, 10, 10])\n",
      "Shape after col attn residual:  torch.Size([16, 8, 10, 10])\n",
      "Shape after concatenation:  torch.Size([16, 16, 10, 10])\n",
      "Shape after concat layer norm:  torch.Size([16, 16, 10, 10])\n",
      "Shape after pre_ffn modulate:  torch.Size([16, 16, 10, 10])\n",
      "Shape after ffn:  torch.Size([16, 16, 10, 10])\n",
      "Shape after post ffn modulate:  torch.Size([16, 16, 10, 10])\n",
      "Shape after residual connection:  torch.Size([16, 16, 10, 10])\n",
      "Shape after downsample:  torch.Size([16, 4, 10, 10])\n",
      "Feature map shape:  torch.Size([16, 4, 10, 10])\n",
      "Feature map shape after input layer norm:  torch.Size([16, 4, 10, 10])\n",
      "Shape after pre_attn modulate:  torch.Size([16, 4, 10, 10])\n",
      "Shape after row attention:  torch.Size([16, 4, 10, 10])\n",
      "Shape after post row attention:  torch.Size([16, 4, 10, 10])\n",
      "Shape after col attention:  torch.Size([16, 4, 10, 10])\n",
      "Shape after post col attention:  torch.Size([16, 4, 10, 10])\n",
      "Shape after row attn residual:  torch.Size([16, 4, 10, 10])\n",
      "Shape after col attn residual:  torch.Size([16, 4, 10, 10])\n",
      "Shape after concatenation:  torch.Size([16, 8, 10, 10])\n",
      "Shape after concat layer norm:  torch.Size([16, 8, 10, 10])\n",
      "Shape after pre_ffn modulate:  torch.Size([16, 8, 10, 10])\n",
      "Shape after ffn:  torch.Size([16, 8, 10, 10])\n",
      "Shape after post ffn modulate:  torch.Size([16, 8, 10, 10])\n",
      "Shape after residual connection:  torch.Size([16, 8, 10, 10])\n",
      "Shape after downsample:  torch.Size([16, 1, 10, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "The shape of the mask [16, 10, 10] at index 1 does not match the shape of the indexed tensor [16, 1, 10, 10] at index 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 140\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    137\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdl)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mautocast():\n\u001b[0;32m--> 140\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_accumulate_every\n\u001b[1;32m    142\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/anaconda3/envs/edge/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/edge/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 352\u001b[0m, in \u001b[0;36mGaussianDiffusion.forward\u001b[0;34m(self, img, *args, **kwargs)\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m h \u001b[38;5;241m==\u001b[39m img_size[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m w \u001b[38;5;241m==\u001b[39m img_size[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheight and width of image must be \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimg_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    350\u001b[0m t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps, (b,), device\u001b[38;5;241m=\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mlong()\n\u001b[0;32m--> 352\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp_losses\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 329\u001b[0m, in \u001b[0;36mGaussianDiffusion.p_losses\u001b[0;34m(self, x_start, t, noise, offset_noise_strength)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;66;03m# if doing self-conditioning, 50% of the time, predict x_start from current set of times\u001b[39;00m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;66;03m# and condition with unet with that\u001b[39;00m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;66;03m# this technique will slow down training by 25%, but seems to lower FID significantly\u001b[39;00m\n\u001b[1;32m    325\u001b[0m \n\u001b[1;32m    326\u001b[0m \u001b[38;5;66;03m# predict and take gradient step\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;66;03m# print(x.shape, t.shape)\u001b[39;00m\n\u001b[1;32m    328\u001b[0m model_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(x, t)\n\u001b[0;32m--> 329\u001b[0m \u001b[43mmodel_out\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpred_noise\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    332\u001b[0m     target \u001b[38;5;241m=\u001b[39m noise[:, \u001b[38;5;241m0\u001b[39m, :, :]\n",
      "\u001b[0;31mIndexError\u001b[0m: The shape of the mask [16, 10, 10] at index 1 does not match the shape of the indexed tensor [16, 1, 10, 10] at index 1"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DiT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
